<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="googlebe1c0a79014759f2.html"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Yilin Liu at SFU CS </title> <meta name="author" content="Yilin Liu"> <meta name="description" content="The homepage of Yilin Liu (刘奕林) at SFU CS. "> <meta name="keywords" content="Yilin Liu; 刘奕林; SFU; Shenzhen University; Computer Graphics; MVS Reconstruction"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8E%83&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yilinliu77.github.io/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?8b4007b6a6e0a73df09a3389d6f49210"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Yilin Liu at SFU CS </h1> <p class="desc">whatsevenlyl@gmail.com / <a href="https://yilinliu77.github.io/assets/pdf/cv.pdf" target="_blank">CV</a> / <a href="https://scholar.google.com/citations?user=dIprnogAAAAJ&amp;hl=en" target="_blank" rel="external nofollow noopener">Google Scholar</a></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-480.webp 480w,/assets/img/self-800.webp 800w,/assets/img/self-1400.webp 1400w," sizes="(min-width: 800px) 231.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/self.jpg?d52a07d7daab1586ef7c593e8e791d1f" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="self.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>I am an AI Research Scientist at <a href="https://www.research.autodesk.com/research-areas/science/ai-lab/" rel="external nofollow noopener" target="_blank">Autodesk AI Lab</a>, working on CAD generation and reconstruction. Before that, I did 2 year PhD study in <a href="https://gruvi.cs.sfu.ca" rel="external nofollow noopener" target="_blank">GrUVi</a> at <a href="https://www.sfu.ca" rel="external nofollow noopener" target="_blank">Simon Fraser University</a>, under the supervision of <a href="https://www.cs.sfu.ca/~haoz" rel="external nofollow noopener" target="_blank">Prof. Hao (Richard) Zhang</a>. I obtained my master’s degree in Computer Science in <a href="https://vcc.tech/index.html" rel="external nofollow noopener" target="_blank">VCC</a> at <a href="https://en.szu.edu.cn/" rel="external nofollow noopener" target="_blank">Shenzhen University</a>, under the supervision of <a href="https://vcc.tech/~huihuang" rel="external nofollow noopener" target="_blank">Prof. Hui Huang</a>. I also hold a Bachelor’s degree in Software Engineering from <a href="https://en.scu.edu.cn/" rel="external nofollow noopener" target="_blank">Sichuan University</a>.</p> <p>My research focuses on CAD reconstruction and generation, learning-based 3D modeling and path planning for MVS reconstruction.</p> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Mar 03, 2025</th> <td> Start work as an AI Research Scientist at Autodesk AI Lab. </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 03, 2024</th> <td> Start internship as an AI researcher at Autodesk AI Lab </td> </tr> <tr> <th scope="row" style="width: 20%">May 20, 2024</th> <td> One paper at SIGGRAPH 2024 journal track </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 23, 2022</th> <td> Graduate Dean’s Entrance Scholarship (GDES) from SFU </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 15, 2021</th> <td> The First Prize Scholarship from Shenzhen University </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4 preview" style="display: flex; justify-content: center; flex-direction: column;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/house_wireframe-480.webp 480w,/assets/img/publication_preview/house_wireframe-800.webp 800w,/assets/img/publication_preview/house_wireframe-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/house_wireframe.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="house_wireframe.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="Ma2024wireframe" class="col-sm-8"> <div class="title">Generating 3D House Wireframes with Semantics</div> <div class="author"> Xueqi Ma ,  <em>Yilin Liu</em> ,  Wenjun Zhou ,  Ruowei Wang ,  and  Hui Huang </div> <div class="periodical"> <em>European Conference on Computer Vision</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://vcc.tech/research/2024/3DWire" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p> We present a new approach for generating 3D house wireframes with semantic enrichment using an autoregressive model. Unlike conventional generative models that independently process vertices, edges, and faces, our approach employs a unified wire-based representation for improved coherence in learning 3D wireframe structures. By re-ordering wire sequences based on semantic meanings, we facilitate seamless semantic integration during sequence generation. Our two-phase technique merges a graph-based autoencoder with a transformer-based decoder to learn latent geometric tokens and generate semantic-aware wireframes. Through iterative prediction and decoding during inference, our model produces detailed wireframes that can be easily segmented into distinct components, such as walls, roofs, and rooms, reflecting the semantic essence of the shape. Empirical results on a comprehensive house dataset validate the superior accuracy, novelty, and semantic fidelity of our model compared to existing generative models. More results and details can be found on https://vcc.tech/research/2024/3DWire. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview" style="display: flex; justify-content: center; flex-direction: column;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/nv-480.webp 480w,/assets/img/publication_preview/nv-800.webp 800w,/assets/img/publication_preview/nv-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/nv.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="nv.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="liu2024nvd" class="col-sm-8"> <div class="title">Split-and-Fit: Learning B-Reps via Structure-Aware Voronoi Partitioning</div> <div class="author"> <em>Yilin Liu</em> ,  Jiale Chen ,  Shanshan Pan ,  Daniel Cohen-Or ,  Hao Zhang ,  and  Hui Huang </div> <div class="periodical"> <em>ACM Trans. on Graphics (Proc. of SIGGRAPH)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://youtu.be/omS38sAbt9w" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/yilinliu77/NVDNet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/yilinliu77/NVDNet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p> We introduce a novel method for acquiring boundary representations (B-Reps) of 3D CAD models which involves a two-step process: it first applies a spatial partitioning, referred to as the “split“, followed by a “fit“ operation to derive a single primitive within each partition. Specifically, our partitioning aims to produce the classical Voronoi diagram of the set of ground-truth (GT) B-Rep primitives. In contrast to prior B-Rep constructions which were bottom-up, either via direct primitive fitting or point clustering, our Split-and-Fit approach is top-down and structure-aware, since a Voronoi partition explicitly reveals both the number of and the connections between the primitives. We design a neural network to predict the Voronoi diagram from an input point cloud or distance field via a binary classification. We show that our network, coined NVD-Net for neural Voronoi diagrams, can effectively learn Voronoi partitions for CAD models from training data and exhibits superior generalization capabilities. Extensive experiments and evaluation demonstrate that the resulting B-Reps, consisting of parametric surfaces, curves, and vertices, are more plausible than those obtained by existing alternatives, with significant improvements in reconstruction quality. Code will be released on https://github.com/yilinliu77/NVDNet. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview" style="display: flex; justify-content: center; flex-direction: column;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/dronerecon-480.webp 480w,/assets/img/publication_preview/dronerecon-800.webp 800w,/assets/img/publication_preview/dronerecon-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/dronerecon.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dronerecon.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="DroneRecon" class="col-sm-8"> <div class="title">Learning Reconstructability for Drone Aerial Path Planning</div> <div class="author"> <em>Yilin Liu</em> ,  Liqiang Lin ,  Yue Hu ,  Ke Xie ,  Chi-Wing Fu ,  Hao Zhang ,  and  Hui Huang </div> <div class="periodical"> <em>ACM Trans. on Graphics (Proc. of SIGGRAPH Asia)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://vcc.tech/research/2022/DroneRecon" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>We introduce the first learning-based reconstructability predictor to improve view and path planning for large-scale 3D urban scene acquisition using unmanned drones. In contrast to previous heuristic approaches, our method learns a model that explicitly predicts how well a 3D urban scene will be reconstructed from a set of viewpoints. To make such a model trainable and simultaneously applicable to drone path planning, we simulate the proxy-based 3D scene reconstruction during training to set up the prediction. Specifically, the neural network we design is trained to predict the scene reconstructability as a function of the proxy geometry, a set of viewpoints, and optionally a series of scene images acquired in flight. To reconstruct a new urban scene, we first build the 3D scene proxy, then rely on the predicted reconstruction quality and uncertainty measures by our network, based off of the proxy geometry, to guide the drone path planning. We demonstrate that our data-driven reconstructability predictions are more closely correlated to the true reconstruction quality than prior heuristic measures. Further, our learned predictor can be easily integrated into existing path planners to yield improvements. Finally, we devise a new iterative view planning framework, based on the learned reconstructability, and show superior performance of the new planner when reconstructing both synthetic and real scenes.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview" style="display: flex; justify-content: center; flex-direction: column;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/urban3d-480.webp 480w,/assets/img/publication_preview/urban3d-800.webp 800w,/assets/img/publication_preview/urban3d-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/urban3d.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="urban3d.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="UrbanScene3D" class="col-sm-8"> <div class="title">Capturing, Reconstructing, and Simulating: the UrbanScene3D Dataset</div> <div class="author"> Liqiang Lin ,  <em>Yilin Liu</em> ,  Yue Hu ,  Xingguang Yan ,  Ke Xie ,  and  Hui Huang </div> <div class="periodical"> <em>European Conference on Computer Vision</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://vcc.tech/UrbanScene3D" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p> We present UrbanScene3D, a large-scale data platform for research of urban scene perception and reconstruction. UrbanScene3D contains over 128k high-resolution images covering 16 scenes including large-scale real urban regions and synthetic cities with 136 km^2 area in total. The dataset also contains high-precision LiDAR scans and hundreds of image sets with different observation patterns, which provide a comprehensive benchmark to design and evaluate aerial path planning and 3D reconstruction algorithms. In addition, the dataset, which is built on Unreal Engine and Airsim simulator together with the manually annotated unique instance label for each building in the dataset, enables the generation of all kinds of data, e.g., 2D depth maps, 2D/3D bounding boxes, and 3D point cloud/mesh segmentations, etc. The simulator with physical engine and lighting system not only produce variety of data but also enable users to simulate cars or drones in the proposed urban environment for future research. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview" style="display: flex; justify-content: center; flex-direction: column;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/dronefly-480.webp 480w,/assets/img/publication_preview/dronefly-800.webp 800w,/assets/img/publication_preview/dronefly-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/dronefly.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dronefly.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="DroneFly" class="col-sm-8"> <div class="title">Aerial Path Planning for Online Real-Time Exploration and Offline High-Quality Reconstruction of Large-Scale Urban Scenes</div> <div class="author"> <em>Yilin Liu</em> ,  Ruiqi Cui ,  Ke Xie ,  Minglun Gong ,  and  Hui Huang </div> <div class="periodical"> <em>ACM Trans. on Graphics (Proc. of SIGGRAPH Asia)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://vcc.tech/research/2021/DroneFly" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Existing approaches have shown that, through carefully planning flight trajectories, images captured by Unmanned Aerial Vehicles (UAVs) can be used to reconstruct high-quality 3D models for real environments. These approaches greatly simplify and cut the cost of large-scale urban scene reconstruction. However, to properly capture height discontinuities in urban scenes, all state-of-the-art methods require prior knowledge on scene geometry and hence, additional prepossessing steps are needed before performing the actual image acquisition flights. To address this limitation and to make urban modeling techniques even more accessible, we present a real-time explore-and-reconstruct planning algorithm that does not require any prior knowledge for the scenes. Using only captured 2D images, we estimate 3D bounding boxes for buildings on-the-fly and use them to guide online path planning for both scene exploration and building observation. Experimental results demonstrate that the aerial paths planned by our algorithm in realtime for unknown environments support reconstructing 3D models with comparable qualities and lead to shorter flight air time.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview" style="display: flex; justify-content: center; flex-direction: column;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/urban3d-480.webp 480w,/assets/img/publication_preview/urban3d-800.webp 800w,/assets/img/publication_preview/urban3d-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/urban3d.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="urban3d.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="UrbanScene3E" class="col-sm-8"> <div class="title">UrbanScene3D: A Large Scale Urban Scene Dataset and Simulator</div> <div class="author"> <em>Yilin Liu</em> ,  Fuyou Xue ,  and  Hui Huang </div> <div class="periodical"> <em>Graphic Open Source Dataset Award</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="https://vcc.tech/UrbanScene3D" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview" style="display: flex; justify-content: center; flex-direction: column;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/dronescan-480.webp 480w,/assets/img/publication_preview/dronescan-800.webp 800w,/assets/img/publication_preview/dronescan-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/dronescan.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dronescan.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="DroneScan20" class="col-sm-8"> <div class="title">Offsite Aerial Path Planning for Efficient Urban Scene Reconstruction</div> <div class="author"> Xiaohui Zhou ,  Ke Xie ,  Kai Huang ,  <em>Yilin Liu</em> ,  Yang Zhou ,  Minglun Gong ,  and  Hui Huang </div> <div class="periodical"> <em>ACM Trans. on Graphics (Proc. of SIGGRAPH Asia)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://vcc.tech/research/2020/DroneScan" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>With rapid development in UAV technologies, it is now possible to reconstruct large-scale outdoor scenes using only images captured by low-cost drones. The problem, however, becomes how to plan the aerial path for a drone to capture images so that two conflicting goals are optimized: maximizing the reconstruction quality and minimizing mid-air image acquisition effort. Existing approaches either resort to pre-defined dense and thus inefficient view sampling strategy, or plan the path adaptively but require two onsite flight passes and intensive computation in-between. Hence, using these methods to capture and reconstruct large-scale scenes can be tedious. In this paper, we present an adaptive aerial path planning algorithm that can be done before the site visit. Using only a 2D map and a satellite image of the to-be-reconstructed area, we first compute a coarse 2.5D model for the scene based on the relationship between buildings and their shadows. A novel Max-Min optimization is then proposed to select a minimal set of viewpoints that maximizes the reconstructability under the the same number of viewpoints. Experimental results on benchmark show that our planning approach can effectively reduce the number of viewpoints needed than the previous state-of-the-art method, while maintaining comparable reconstruction quality. Since no field computation or a second visit is needed, and the view number is also minimized, our approach significantly reduces the time required in the field as well as the off-line computation cost for multi-view stereo reconstruction, making it possible to reconstruct a large-scale urban scene in a short time with moderate effort.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview" style="display: flex; justify-content: center; flex-direction: column;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/vgf-480.webp 480w,/assets/img/publication_preview/vgf-800.webp 800w,/assets/img/publication_preview/vgf-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/vgf.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="vgf.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="VGFNet21" class="col-sm-8"> <div class="title">VGF-Net: Visual-Geometric Fusion Learning for Simultaneous Drone Navigation and Height Mapping</div> <div class="author"> <em>Yilin Liu</em> ,  Ke Xie ,  and  Hui Huang </div> <div class="periodical"> <em>Graphical Models</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://vcc.tech/research/2021/VGFNet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>The drone navigation requires the comprehensive understanding of both visual and geometric information in the 3D world. In this paper, we present a Visual-Geometric Fusion Network (VGF-Net), a deep network for the fusion analysis of visual/geometric data and the construction of 2.5D height maps for simultaneous drone navigation in novel environments. Given an initial rough height map and a sequence of RGB images, our VGF-Net extracts the visual information of the scene, along with a sparse set of 3D keypoints that capture the geometric relationship between objects in the scene. Driven by the data, VGF-Net adaptively fuses visual and geometric information, forming a unified Visual-Geometric Representation. This representation is fed to a new Directional Attention Model (DAM), which helps enhance the visual-geometric object relationship and propagates the informative data to dynamically refine the height map and the corresponding keypoints. An entire end-to-end information fusion and mapping system is formed, demonstrating remarkable robustness and high accuracy on the autonomous drone navigation across complex indoor and large-scale outdoor scenes.</p> </div> </div> </div> </li> </ol> </div> <h2> <a href="/" style="color: inherit">Service</a> </h2> <p> Reviewer in SIGGRAPH Asia, Eurographics, ACM Transaction on Graphics </p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Yilin Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: March 05, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>